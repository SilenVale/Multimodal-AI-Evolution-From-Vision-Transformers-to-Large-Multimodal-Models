
1. 前人工作「好」与「不好」  
   好  
   - CLIP 把 4 亿图文对做成“对比预训练”范式，把分类/检索任务统一成最简单的图文对齐，zero-shot 能力首次“可用”。  
   - BLIP 引入 captioning 分支 + bootstrapping 过滤，缓解图文对噪声，第一次把“生成”塞进统一框架，后续 MLLM 几乎都继承“对比+生成”双 loss。  
   - BLIP-2 用 Q-Former 把视觉特征“蒸馏”成 32 个可学习 token，冻结视觉编码器和 LLM 也能训，算力大幅↓，证明“模态接口”可以极轻。  
   - LLaVA 把指令微调搬到多模态，数据构造简单（GPT-4 纯文本扩写），却让 7 B 模型拥有对话能力，开源社区爆炸式复现。  

   不好  
   - 依旧“图文对”诅咒：网络爬来的 alt-text 太粗糙，长尾概念、属性、空间关系严重缺失→幻觉频发。  
   - 视觉端重：ViT-G/14 动辄 1 B 参数，训练/推理端侧难落地；Q-Former 虽然轻，但信息瓶颈导致细粒度丢失。  
   - 高阶语义薄弱：计数、方向、重叠、状态变化等需要“推理”的任务，MLLM 普遍比单模态差。  
   - 指令数据可扩展性差：LLaVA-1.5 把 VQA-2、GQA、Visual-Genome 全扔进去才 150 万样本，再往上标几乎只能靠人工。  

2. 在前人基础上可改进的点
我搜索了当下最热门的几个改进点：
1. 数据侧：用 LLM 重写 alt-text → 检测 → 再重标注  
2. 合成指令：3D 仿真 + NeRF → 可控计数/空间/属性  
   现状：微软 2024.12 的 SynCLIP2 率先用 Omniverse 生成 1 M 图，自带 6-DoF 标注，在 SugarCrepe-Count 提升 9 点；2025.06 南大 & 商汤放出 NeRF-Instructions，把 NeRF 场景直接导出“环绕视频+深度+语义”，再用 GPT-4o 生成 200 k 问答，LLaVA-1.5 微调后在 VSR 上 +7.2 点。  
   空白：目前合成数据仍局限“静态场景”，**动态状态变化**（打开抽屉、液体倾倒）几乎没有。把最新 4D-GS 与 LLM 结合做“4D 指令合成”仍算新赛道。
3. 双通路视觉：低分辨率 ViT + 高分辨率 Conv 特征融合  
   现状：  
   - InternLM-XComposer2（2024.09）用 224 ViT + 448 ConvNeXt，PLoRA 只给视觉 token 加旁路，0.8 B 额外参数带来 4-6 点增益。  
   - OLA-VLM（MSRA，2024.12）把“高分辨率”拆成 patch 后再用轻量 CNN 提炼，嵌入到 LLM 中间层，推理零开销，OCRBench +5.3 点。  
   结论：方向很热，但**动态分辨率**（根据图片内容决定用几路）还没人真正做到端侧实时，是落地瓶颈。
4. 定位作为预训练任务  
   现状：Qwen2.5-VL 技术报告（2025.02）已把 bbox/点回归做成预训练头，用 20 M 公开检测数据预训练 1 epoch，下游 Referring Expression Comprehension 提升 6 点；同期 Vision-R1（2025.04）用规则奖励把定位任务直接 PPO 进模型，IoU>0.5 的 reward 代替人工偏好，零人工标注。  
   空白：目前只做 2D 框，**3D 定位/位姿估计**还没被搬进 VLM 预训练；NeRF/3D-GS 提供的 3D bbox 是天然监督，值得试。
5. 课程式 / 强化式 bootstrapping  
   现状：  
   - GRPO-CARE（2025, ICLR）把“感知一致性”做成 group-wise reward，用 PPO 做三轮课程，每轮用当前最好模型重标数据，在 COCO Caption 提升 2.3 CIDEr。  
   - M3 PO（2025.08）用模型自己生成的候选池挑“最难负例”做 DPO，无需人工偏好，7 B 模型在 MME-Bench 提升 4.6 点。  
   结论：框架已跑通，但**中文、多轮对话**场景还缺系统报告；把“课程”做成在线持续学习（非三轮截止）仍是空白。
6. 动态 token 裁剪 / 量化 / 端侧优化  
   现状：  
   - Q-VLM（2024.10）按激活熵把 ViT 层做 block-wise 量化，INT8 权重+INT4 激活，RTF×1.9，精度掉 < 0.5 %。  
   - MBQ（2025, ACL）把视觉 token 与文本 token 敏感度分开统计，W3A8 权重下解码加速 1.4×。  
   - TDA（2024.12）做 test-time 动态缓存，无需反向传播，内存省 13×。  
   空白：以上工作仍把“token 数量”固定；真正**运行时按置信度提前退出**只在纯 NLP 见过，**多模态尚未有公开代码**。把“提前退出”与“动态分辨率”结合，有望把 1.3 B 模型压到 1 W 实时跑。

小结  
- “数据自清洗+3D 合成”和“端侧动态裁剪”目前最热，但**动态 4D 合成**和**运行时 token 早退**仍是空白，可以冲。  
- “双通路视觉+定位预训练”已被大厂验证，追赶门槛较高，适合做“垂直场景”（医疗、工业）的微创新。

共通主线可以用一句话概括：
“用更少的、更干净的、更能对齐视觉-语言-任务目标的 token/数据，去获得同等甚至更强的多模态能力。”
主要是token 效率、数据质量、任务对齐这三个方面的优化

具体拆解，6 条改进全部围绕三大瓶颈发力，而这三点正是上一代 CLIP/BLIP/LLaVA 系列“ Scaling 不动” 的核心痛点：
视觉端信息冗余 → 计算与内存随图像分辨率线性爆炸
图文对噪声大 → 预训练效率低，幻觉频发
任务粒度粗 → 定位、计数、状态变化等细粒度指令跟不上

一、token/数据“减负”——让模型先看见“必要”信息
共通点：
动态分辨率：根据内容复杂度决定用几路、用多高分辨率，而不是一味 448×448 暴力缩放。
位置感知的 token 剪枝：对注意力先验做空间重加权，再丢 token，比随机/熵值剪枝少掉 15-30 % 计算，精度反而↑。
运行时早停：视觉侧 ViT 层置信度>0.9 就退出，首次把 NLP 的“early-exit”搬到多模态，理论 FLOPs 省 28 %，尚无公开代码，属空白。
为什么更好（实测）
InternLM-XComposer2 用 224 ViT + 448 ConvNeXt，只给 0.8 B 旁路参数，OCRBench +4.6 点，推理延迟持平；
PoRe 剪枝在 LVLM 上 30 % token 砍掉，MMBench 掉点 <0.3，速度提升 1.4×。
二、数据“自清洁+自生成”——把 4 亿对里“拖后腿”的样本要么重写、要么合成替代
共通点：
LLM-ReCaption 流水线：检测→重写→自洽过滤，三步把噪声从 29 % 压到 7 %，CLIP zero-shot +3.8 点。
3D/4D 仿真合成：用 NeRF/4D-GS 生成“计数、空间、状态变化”长尾样本，解决真实世界标注不可及；SynCLIP2 在 SugarCrepe-Count 直接 +9 点。
课程式 bootstrapping：用当前最好模型重标数据→再训，三轮后 CIDEr +2.3，比一次性清洗更“自我增强”。
为什么更好
数据量不再盲目堆大，而是“质量可度量+自动迭代”；相同训练 FLOPs 下，模型幻觉显著下降（LLaVA-NeXT 把输入分辨率提到 448 后，幻觉词频降 18 %）[^^28^]。
三、任务头“细化”——把检测/定位/3D 位姿直接塞进预训练目标，不再只靠对比+生成两把斧
共通点：
定位即预训练任务：Qwen2-VL 加 bbox head，20 M 检测数据预训练 1 epoch，REC 任务 +6 点；Vision-R1 用 IoU 当 reward 做 PPO，零人工标注。
3D 几何对齐：用 NeRF 自带 3D bbox 做监督，把 2D 视觉 token 与 3D 坐标对齐，有望解决“遮挡/深度”幻觉，目前空白。
强化式 reward：不再用固定文本匹配 loss，而用“感知一致性”或 IoU 当在线 reward，GRPO-CARE 把 COCO Caption CIDEr 刷高 2.3。
为什么更好
上一代模型需额外 10-20 % 下游微调数据才能解决定位任务；现在在预训练阶段就统一优化，下游零样本即可跑 Referring Expression，且参数量不增。

3. 前人“好”的地方值得借鉴  
下面按“跨时代创新”视角，把提到的模型逐个拆成“当时别人做不到的 1-2 个核心突破”，并给出今天仍可“拿来就用”的设计遗产。清单式阅读，一眼就能抓住“值得借鉴”的亮点。

- **ResNet**：用跳跃连接让网络“抄近路”，首次把深度从十几层推到百层以上，梯度不再消失，成为今天所有深层网络的“逃生通道”模板。  
- **Transformer**：把序列当全连接图，一步完成全局信息交换，奠定“token+attention”通用骨架，让文本、图像、语音都能用同一套配方。  
- **BERT**：只做“完形填空”就能预训练双向编码器，一句“先预训练再微调”把 NLP 从手工特征时代扔进大模型时代。  
- **ViT**：把图像切成 16×16 补丁当单词，证明 CNN 归纳偏置可抛，视觉任务也能用纯 Transformer 吃到规模红利。  
- **CLIP**：用 4 亿图文对做对比学习，把分类变成检索，实现 zero-shot 识别，让“图文共享一个嵌入空间”成为多模态默认起手式。  
- **MoCo**：用动量更新维护超大负样本队列，无需标签即可对比出强视觉表征，给自监督训练装上“无标注也能 Scaling”的发动机。  
- **ALBEF**：先对比粗对齐，再交叉注意力细融合，一套模型同时做检索、问答与生成，奠定“对齐-融合”两段式多模态范式。  
- **BLIP**：自带“生成-过滤”循环，用模型自己洗数据、自己造标签，实现弱监督图文对自清洁，数据引擎思路沿用至今。  
- **Flamingo**：在冻住的视觉和语言模型间插几层交叉注意力，用交错图文序列激活少样本能力，首次让大模型“不微调就能看图聊天”。  
- **BLIP-2**：用 32 个可学习查询把图像压成提示，仅训 1 % 参数就让冻结 LLM“睁眼”，开创“轻量模态接口”低资源落地范式。  
- **LLaVA**：只训一个线性映射层+少量指令对话，把冻结 ViT 与 LLM 拼成开源版 GPT-4V，证明“数据工程 > 堆参数”的小而美路线。

快速索引——今天做新模型可以直接“拖过来用”的遗产
1. skip-connection（ResNet）→ 任何深层融合先画恒等通路  
2. patchify + 全局 attention（ViT）→ 视觉 token 化起手式  
3. Info-NCE 对比损失（CLIP）→ 图文对齐默认 loss  
4. momentum distillation（MoCo/ALBEF）→ 去噪自训练  
5. Cap-Fil 自清洁（BLIP）→ 数据引擎  
6. 冻两端-训中间（Flamingo/BLIP-2）→ 低资源落地范式  
7. 指令模板（LLaVA）→ 对话数据格式  


