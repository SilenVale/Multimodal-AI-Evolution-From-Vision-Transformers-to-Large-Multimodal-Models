ResNet
用“跳跃连接”把输入直接加到卷积输出上，缓解深层网络梯度消失，可训练上百层仍稳定收敛。
需分层抽象的视觉任务：图像分类、目标检测、语义分割、人脸识别、医学影像、工业质检。
Transformer
输出就是一组与输入序列等长的“上下文向量”，把每个 token 的原始词义/像素义变成了融合全局信息后的新表示。
上下文感知，每个 token 的向量已融合整句/整图信息；
全局交互，任意两位置可直接交互，距离常数 = 1，高维稠密且线性可分；
BERT
“用大规模无标注文本玩‘完形填空’和‘上下句接龙’，先预训练一个双向 Transformer，再拿它的向量去微调任何下游任务。
ViT
用 Transformer 替代 CNN，把图像当序列做全局自注意力，提取单模态视觉特征。
需长距离关系的纯视觉任务：图像分类、检测、分割、医学影像、遥感。
CLIP
把图像和文本映射到同一语义空间，用对比学习让“图文对齐”，实现零样本分类。
无标注数据下的零样本/少样本分类、跨模态检索、文生图、图生文
MoCo
用“队列 + 动量更新”做自监督视觉对比学习，无需文本即可练出可迁移视觉表征。
数据稀缺且无文本的纯视觉预训练，后续接检测/分割/分类微调。
ALBEF
先图文对比学习对齐，再用交叉注意力融合，兼顾理解与生成，性能比 CLIP 更精细。
需要精细图文交互的 VQA、图像字幕、检索、视觉推理等多模态下游任务。
总结：“ViT 给视觉 Transformer 打地基；CLIP 让图文对齐做零样本；MoCo 不用文本也能自监督出视觉特征；ALBEF 在 CLIP 基础上加交叉注意力，做更细的图文理解。”
BLIP
“把 captioning 与 filtering 做成一个统一框架：先用 bootstrap 方式自动生成+过滤图文对，再用多任务 Transformer 同时学理解（image-text contrastive & matching）和生成（language modeling），兼顾编码器与解码器。”
需大规模弱监督图文数据预训练、且最终要支持图像字幕、VQA、检索、视觉推理等“既看又写”的多模态场景。
Flamingo
在冻结的 ViT 与 LLM 之间插入可学习的交叉注意力插槽，把任意数量、任意位置的图像/视频帧当作稀疏提示，让大模型无需微调就能看图聊天；用大规模图文交错数据做少样本上下文学习，实现开放域视觉对话、OCR、计数、视觉推理一键通。
需要即插即用的视觉语言能力：多轮图文对话、交互式问答、长视频理解、文档图像分析、机器人指令解析等快速适应新任务而不更新参数的场合。
BLIP-2
在冻结的 ViT 与冻结的大语言模型之间插一层轻量级 Q-Former，用可学习查询向量把图像特征压缩成固定长度提示，再交给 LLM 做文本生成；两阶段预训练先对齐视觉-查询，再对齐查询-语言，只训 1% 参数就能让大模型‘睁眼’。
已存在现成 ViT 与 LLM，不想端到端微调；
需要图文对话、图像字幕、VQA、检索等多模态任务“零样本”一键通；
计算资源有限，希望用小成本桥接获得大模型视觉语言能力。
LLaVA
把冻结的 ViT 视觉编码器接到冻结的 LLaMA 大模型，中间只训一个线性投影层，用少量图文指令数据微调，让大模型“看见”图像就能多轮对话。
开源低成本替代 GPT-4V，适用于视觉问答、图像字幕、多轮图文聊天、文档图理解、教学辅导、机器人指令解析等即插即用场景。 

MOCO(基于动量对比的无监督视觉表示学习)
 概念：将对比学习建模为一个字典查找任务，通过维护一个动态字典（包含正样本和负样本的特征），来训练编码器提取图像特征。
 MoCo 的关键创新包括：
队列机制：使用一个先进先出队列存储大量负样本特征，突破 batch size 限制；
动量编码器：引入一个动量更新的编码器，保证特征一致性，避免不同 batch 间特征漂移；
InfoNCE 损失函数：通过最大化正样本对的相似度、最小化负样本对的相似度，学习判别性特征。
MoCo 在 ImageNet 等任务上表现优异，甚至超过了当时的监督学习方法，展示了无监督学习的巨大潜力
特点：
无需人工标注标签：对比学习不使用人为标注的类别标签，而是利用数据本身的结构或变换来构造正负样本
正负样本由规则自动生成：虽然没有标签，但对比学习通过自定义规则（如数据增强、视图一致性）自动生成正负样本对，从而构建监督信号。这种方式属于自监督学习，而自监督学习是无监督学习的一种形式
目标是学习特征表示，而非分类：
对比学习的核心是学习图像的表征（representation），而不是直接进行分类或回归。学到的特征可以迁移到下游任务中，通过微调实现分类、检测等目标

BERT
拆开就是三步：
模型：深堆 Transformer Encoder，真正双向同时看左右上下文。
预训练：自监督——
MLM（掩码语言模型）：随机把 15 % 的词换成 [MASK]，让网络猜原词；
NSP（下一句预测）：给两句话，判断它们是否连得上。
微调：把预训练参数直接接到具体任务（分类、序列标注、问答等），只加一层输出头，用小数据微调即可。
“是否用因果掩码” 就是 BERT（双向）与 Transformer（单向）在自注意力机制层面的唯一根本区别；
| 模型/任务                                       | 用的掩码类型                                 | 是否因果 | 目的          |
| ------------------------------------------- | -------------------------------------- | ---- | ----------- |
| **Transformer Encoder**（BERT、ALBEF 文本塔、ViT） | **无因果掩码**<br>仅有 padding mask           | ✅ 双向 | 一句/一张图内全局可见 |
| **Transformer Decoder**（GPT、解码器）            | **因果掩码**（下三角）                          | ❌ 单向 | 生成时只能看过去    |
| **Encoder-Decoder**（原始机器翻译）                 | Encoder：无因果<br>Decoder：因果<br>Cross：无因果 | 混合   | 编码全局，生成自回归  |



对比学习没有固定的标签，同一个样本生成的特征一直在变化，因此要保证负样本要尽量多，且最好由同一个模型生成因此我们引入指数移动平均动量模型：（EMA）
**“想让队列里的 key 特征尽可能保持一致性，避免训练过程中出现‘分布漂移’导致对比学习失效，所以用动量更新把 key 编码器‘慢半拍’地跟着 query 编码器走。”**

下面把原因拆开讲：

1. 对比学习需要“大量、一致”的负样本  
   MoCo 靠“队列”来存负样本，但队列是**跨迭代复用**的。如果 key 编码器参数在每一步都剧烈变化，那么  
   - 新入队的 key 与队列里旧的 key 就会**分布不一致**；  
   - 模型会把这种差异当成“语义差异”，造成**假负样本/假正样本**，拉低性能。

2. 动量更新 = 低通滤波器  
   动量系数 m≈0.999，意味着  
   θₖ ← 0.999·θₖ + 0.001·θ_q  
   每一步 key 网络只“微微”跟随 query 网络，**参数变化极其缓慢**，从而保证  
   - 新旧 key 特征近似同分布；  
   - 队列里的历史特征仍可“复用”，不会快速过期。

3. 梯度不回传也是同一目的  
   如果允许 key 网络像普通网络那样直接走反向传播，它会被 loss 拉得“大步流星”，同样会破坏一致性。  
   动量更新 + 梯度截断，把 key 网络**从优化器的‘大步’中解放出来**，只当 query 网络的“影子”。

4. 实验验证  
   原论文表 2 显示：  
   - m=0.999 时 ImageNet 线性分类 60.6% top-1；  
   - m=0.9 掉到 58.5%，m=0 掉到 53.2%（完全不用动量）。  
   动量越大，一致性越好，效果越好。

结论  
**动量更新是“以时间换一致性”的 trick**：牺牲一点点 key 网络的“新鲜度”，换来队列里数千个负样本的分布稳定，从而让对比学习可以**放心地复用历史特征**，既省内存又提性能。

具体流程：
MoCo（**Momentum Contrast**）将无监督视觉表征建模为**动态字典查找**任务，流程极简洁：

1. 编码  
   同一张图做两次随机增强，得到 query 视图 \(x_q \) 和 key 视图 \(x_k \)；分别送入**Encoder** \(f_q \) 和**动量 Encoder** \(f_k \)，得到特征向量 \(q \) 与 \(k \)。
   和CLIP类似：CLIP通过**网络** 用“图-文”配对，MoCo 用“图-图”配对；正负样本的构造逻辑（batch 内对角线为正，其余为负）完全一样。

2. 动量更新  
   \(f_k \) 不参与反向传播，其参数按动量方式缓慢追赶 \(f_q \)：  
   \[
   \theta_k \leftarrow m\,\theta_k + (1-m)\,\theta_q,\quad m\approx 0.999
   \]  
   保证字典特征一致性。

3. 字典与对比  
   维护一个**先进先出队列**（容量 65 536），存储最近若干 batch 的 \(k \) 作为负样本；  
   用 InfoNCE 计算损失：  
   \[
   \mathcal L = -\log\frac{\exp(q\cdot k_+/\tau)}{\sum_{i=0}^K \exp(q\cdot k_i/\tau)}
   \]  
   只更新 \(f_q \) 与队列，字典即时刷新。

4. 迭代  
   每个 batch 重复 1-3，训练结束后**仅用 \(f_q \)** 提取通用视觉特征，供下游任务微调。

一句话：**“用动量编码器不断维护一个大型特征字典，让 query 在其中找到匹配的正样本即可学会好的表示。”**

伪代码：
# f_q, f_k: 用于生成 query 和 key 的两个编码器网络
# queue: 一个容量为 K 的队列，保存历史 key 特征 (C×K)
# m: 动量系数，通常设为 0.999
# t: 温度系数，用于调节对比损失

f_k.params = f_q.params          # 1. 初始化：让 key 网络先复制 query 网络的参数

#同一张原始图做两次随机增强 → 得到一对正样本视图和多对负样本
for x in loader:                 # 2. 遍历每个 mini-batch，共 N 张图
    x_q = aug(x)                 # 3. 得到第一张增强视图（query 侧）
    x_k = aug(x)                 # 4. 得到第二张增强视图（key 侧）

    q = f_q.forward(x_q)         # 5. 生成 N 条 query 特征 (N×C)
    k = f_k.forward(x_k)         # 6. 生成 N 条 key  特征 (N×C)
    k = k.detach()               # 7. 切断梯度，防止 key 网络被反向传播更新

    # 8. 计算“正样本”相似度，N×1，每条样本与自己的 key 内积
    l_pos = bmm(q.view(N, 1, C), k.view(N, C, 1))

    # 9. 计算“负样本”相似度，N×K，每条 query 与队列中 K 条历史 key 内积
    l_neg = mm(q.view(N, C), queue.view(C, K))

    # 10. 拼接正负 logits，得到最终 N×(1+K) 的矩阵
    logits = cat([l_pos, l_neg], dim=1)

    # 11. 构造标签：正样本永远在第 0 列，故标签全 0
    labels = zeros(N)

    # 12. 计算 InfoNCE 对比损失，用温度 t 缩放 logits
    loss = CrossEntropyLoss(logits / t, labels)

    loss.backward()              # 13. 反向传播，仅更新 f_q 的参数
    update(f_q.params)           # 14. 执行 SGD/Adam 等优化器一步

    # 15. 动量更新：key 网络参数缓慢跟随 query 网络
    f_k.params = m * f_k.params + (1 - m) * f_q.params
    
    #把不同图产生的 k 不断压入队列；队列里的 K 个历史特征即为当前 batch 的负样本
    enqueue(queue, k)            # 16. 把当前 mini-batch 的 key 特征入队
    dequeue(queue)               # 17. 把最早入队的 mini-batch 出队，保持队列长度固定



### 一、什么是 Contrastive Loss（对比损失）？
**Contrastive Loss** 是一种用于**对比学习**的损失函数，其核心目标是：

> **让相似的样本在特征空间中距离更近，让不相似的样本距离更远。**

它最初被广泛用于**孪生网络（Siamese Network）**中，后来成为对比学习（如 SimCLR、MoCo 等）的基础。

---

### 二、Contrastive Loss 的数学形式

以经典的 Contrastive Loss（Hadsell et al., 2006）为例，其形式如下：

\[
\mathcal{L}(x_1, x_2, y) = y \cdot d^2 + (1 - y) \cdot \max(0, m - d)^2
\]

其中：

- \( x_1, x_2 \)：输入的两个样本；
- \( d = \|f(x_1) - f(x_2)\|_2 \)：两个样本在特征空间中的欧氏距离；
- \( y \in \{0, 1\} \)：标签，1 表示正样本对（相似），0 表示负样本对（不相似）；
- \( m \)：margin（边界值），用于控制负样本之间的距离下限。

---

### 三、Contrastive Loss 的作用机制

| 情况         | 损失项说明                                                                 |
|--------------|------------------------------------------------------------------------------|
| **正样本对（y=1）** | 损失为 \( d^2 \)，鼓励模型让相似样本的特征距离尽可能小。                     |
| **负样本对（y=0）** | 损失为 \( \max(0, m - d)^2 \)，鼓励模型让不相似样本的特征距离大于 margin \( m \)。 |

---

### ✅ 总结一句话：

> **Contrastive Loss 是一种通过“拉近正样本、推远负样本”来训练特征提取模型的损失函数，是对比学习和孪生网络的核心组件之一。**



InfoNCE 是 **“Info Noise-Contrastive Estimation”** 的缩写，它把 **互信息最大化** 问题转化为一个 **多分类交叉熵** 问题，是对比学习的标准损失函数。
---

### 1. 一句话直觉  
“把正样本当成正确类别，把大量负样本当成噪声类别，让模型在『1 个正 vs K 个负』的 softmax 里选对正样本。”

---

### 2. 公式  
给定一个 query 向量 **q** 和一组候选 key 向量 {**k**₀,**k**₁,…,**k**_K}，其中 **k**₀ 是正样本，其余为负样本，温度缩放后的 InfoNCE 损失为

\[
\mathcal L_{\text{InfoNCE}} = -\log\frac{\exp(q\cdot k_+/\tau)}{\sum_{i=0}^{K}\exp(q\cdot k_i/\tau)}
\]

- **τ>0**：温度系数，控制分布尖锐程度。  
- **K**：负样本个数，越大越接近真实互信息。  
- **q·k**：通常用余弦相似度。

---

### 3. 与互信息的关系  
InfoNCE 是互信息 **I(q;k)** 的下界：

\[
I(q;k) \geq \log K - \mathcal L_{\text{InfoNCE}}
\]

最小化 **ℒ** 即最大化 **I(q;k)**，从而让表征保留更多原始输入的信息。

---

### 4. 实战特点  
- 把 **度量学习** 做成了 **分类问题**，无需三元组挖掘。  
- 天然支持 **大批量负样本**（MoCo 用队列、SimCLR 用 large batch）。  
- 温度 **τ** 是关键超参，常用 0.07–0.2。

---

### 5. 一句话总结  
**InfoNCE = 带温度的“1 对 K” softmax 交叉熵，用来最大化 query 与正 key 的互信息，是对比学习的核心损失。**



ALBEF(在融合前对齐：用动量蒸馏的办法进行视觉和语言的表示学习)
ALBEF（Align Before Fuse）的出现，是为了解决早期视觉-语言预训练（VLP）模型在模态对齐、计算成本、数据噪声三方面的核心痛点

多模态学习
任务类型：
文本-图像检索
图像问答
多模态推理
模型结构：
图像编码器，文本编码器，融合编码器

框架：图片\ALBEF-fm.png
在融合前对齐：（Align）
图片和文本encoder部分同clip，只有一点区别就是albef文本部分是bert结构的（双向自注意力），而clip是mask掩码；借着优化对比损失部分，改为采用moco结构提取CLS和正负样本做自监督视觉对比学习

跨模态融合：（Fuse）
多模态融合编码器部分
类比transformer里的交叉自注意力部分，将已对齐的 image token 与 text token 一起送入 6 层多模态编码器（BERT 后 6 层）。每层用 cross-attention（文本 Q，图像 K/V）实现图文交互，输出融合特征。q来自文本编码器，k和v来自图像编码器

多任务预训练：
在对齐与融合阶段同步优化三个损失：
• ITC：图文对比损失（对齐阶段）。
• MLM：掩盖语言建模，用融合特征预测被掩词。
• ITM：图文匹配二分类，用融合 [CLS] 判断图文是否匹配；负样本采用“难例”——batch 内相似度最高的图文对。
这里注意：6 层是为了让图文 token 进行充分、深层、多粒度的信息交换；损失只算一次，但交换过程本身需要深度网络才能实现。“多次更新参数”靠的并不是“每层算一次损失”，而是不断喂新 batch
ALBEF 的三项预训练损失形式如下（无动量蒸馏版本；含动量时再加 KL 项）：
1. ITC loss（Image-Text Contrastive）  
   作用：在融合前把图文 [CLS] 向量拉到同一空间。  
   形式：对称交叉熵 InfoNCE  
   令 s_I2T = softmax( (g_v v_cls)ᵀ (g_w w_cls^+) / τ )  
        s_T2I = softmax( (g_w w_cls)ᵀ (g_v v_cls^+) / τ )  
   标签 y_I2T, y_T2I 为 one-hot 正样本索引  
   L_ITC = ½[ CE(s_I2T, y_I2T) + CE(s_T2I, y_T2I) ]  
   其中 τ 是可学习温度，负样本来自 batch 内其余 (I,T) 对 。

2. MLM loss（Masked Language Modeling）  
   作用：借助图像信息恢复被掩词。  
   形式：标准交叉熵  
   对 15 % 随机 mask 后的文本 Ť，多模态编码器输出 mask 位置 logits p_msk  
   L_MLM = − Σ log p_msk(w_true | I, Ť)  
   仅对图文匹配样本计算 。

3. ITM loss（Image-Text Matching）  
   作用：判断图文对是否匹配。  
   形式：二分类交叉熵  
   取多模态 [CLS] 向量经 FC 得 2-D 概率 p_itm  
   正样本 = 真实对；负样本 = batch 内“最难”对（ITC 相似度最高）  
   L_ITM = − [ y log p_itm + (1−y) log(1−p_itm) ]  
   一个 batch 通常构造 1×B 正 + 2×B 负，共 3B 条样本 。

总结：ITC 先把两种模态“拉到同一间屋子”，ITM 再让它们在屋子里“仔细核对身份证”。两者串行配合，一个对齐表征，一个校验匹配。
总损失  
L = L_ITC + L_MLM + L_ITM  
（若启用动量蒸馏，则 ITC 与 MLM 再各加一项 α·KL(q‖p)）。


动量蒸馏：（MoD）
维护一个参数滑动平均的“动量教师”模型，为 ITC/MLM/ITM 任务生成软伪标签；学生网络在原始标注与伪标签之间做一致性正则化，有效抑制网络噪声并提升性能。
关键思想-伪目标损失
对 ITC 和 MLM 任务，除了与 one-hot 标签计算常规交叉熵外，再加一项：
L_KL = α KL(q ‖ p)
学生网络（θ）被鼓励去模仿老师网络（ξ）输出的软分布 q。
由于 q 是软标签，即使人工标注有误，只要老师预测正确，学生仍能学到正确信号，从而提升鲁棒性。
详细见图片\动量蒸馏.png

下游微调(即模型迁移)
预训练完成后，去掉动量教师，仅保留学生模型。根据任务替换顶层：
• 检索：直接用图文 [CLS] 向量算余弦相似度。
• VQA：在融合 [CLS] 后接答案解码器，自回归生成答案。
• NLVR²：把多模态编码器复制两份，共享权重，分别处理双图，再拼接 [CLS] 做二分类。


BLIP（通过自举方式预训练的语言-图像模型，它统一了视觉-文本的理解和生成）
BLIP（Bootstrapping Language-Image Pre-training）是 Salesforce 研究院 2022 年提出的视觉-语言基础模型，核心思想是“用模型自己生成的优质文本”来清洗和提升噪声图文数据，从而在统一框架内同时完成理解（Understanding）与生成（Generation）任务。
改进点：
训练一个模型既可以做检索又可以做生成，同时解决网络收集图文对数据中的噪声问题
具体区别体现在两方面：
模型能力
ALBEF 是纯双向编码器结构，预训练目标只有 ITC+ITM，没有语言建模损失，因此原生前向推理只能算相似度，不能直接输出句子；要做 caption 需外挂一个解码器并重新微调。
BLIP 的 MED 架构把“单模态编码器→图文融合编码器→图文融合解码器”做成同一组参数，预训练时同时优化 ITC+ITM+LM。结果一个 checkpoint 既能当图文检索编码器，又能当图像描述/问答生成器，无需额外解码器再训练。
数据自举
ALBEF 的“动量蒸馏”只是用动量模型给伪标签，相当于软标签去噪，仍然局限在原始文本上打补丁。
BLIP 额外训练了一个 Captioner 为网络图片写新描述，再用 Filter 从“原始文本+合成文本”里挑高质量对，实现“用模型自己生成的干净文本替代噪声文本”的迭代放大。这一步直接针对“网络文本不准确”这一核心痛点，而 ALBEF 并未涉及文本再生成的清洗策略。

### 1. 架构要点  
- **Multimodal mixture of Encoder-Decoder（MED）**：同一套参数可切换三种角色  （通过共用参数）
  - 单模态编码器（图像/文本各自编码）  
  - 图像引导的文本编码器（双向交叉注意力，用于检索、匹配）  
  - 图像引导的文本解码器（因果注意力，用于生成描述、回答）  
- **ViT 图像编码器** + **Transformer 文本塔**，三阶段预训练目标联合优化：  
  ① 图文对比学习（ITC）  
  ② 图文匹配（ITM）  
  ③ 图像条件语言建模（LM）

### 2. 数据自举机制 CapFilt  
- **Captioner**：在少量人工标注数据上微调后，为网络图片自动生成多条候选描述。  
- **Filter**：判断原始网页文本与合成描述的质量，只保留高置信度图文对，实现“用干净数据训练更强的下一代模型”——即 Bootstrapping。

### 3. 能力与应用  
- **单模型多任务**：图像描述、视觉问答（VQA）、图文检索、视频-文本零样本迁移等。  
- **效果**：在 COCO Caption、Flickr30K、VQA2.0 等基准上取得当年 SOTA，且对视频任务无需重新训练即可泛化。

### 4. 系列演进  
- BLIP-2 引入 Q-Former 桥接冻结 ViT 与冻结大语言模型，实现“用 LLM 看懂图”。  
- BLIP-3（2025）进一步用可扩展的 Vision Token Sampler 替代 Q-Former，支持任意分辨率输入与纯自回归训练，参数量达 4 B，并开源系列模型与数据。

一句话总结：BLIP 通过“自举式数据清洗 + 统一编码器-解码器”让视觉-语言模型在理解与生成任务上同时受益，后续版本持续把模型做大、做清晰、做开源。

补充：
LM 和 MLM 的核心区别就是“**预测方向**”和“**训练目标**”不一样：
| 维度 | LM（Language Modeling） | MLM（Masked Language Modeling） |
|---|---|---|
| 预测方式 | **从左到右**（因果/自回归）（causal/autoregressive） | **双向**（bidirectional） |
| 可见上下文 | 只能看到**左侧**已生成的词 | 可以看到**左右两侧**所有词 |
| 训练目标 | 逐词生成下一个 token，最大化 \(P(w_t|w_{<t})\) | 随机遮住部分 token，最大化 \(P(w_{\text{mask}}|w_{\text{ctx}})\) |
| 代表模型 | GPT 系列、BLIP 的解码器、LLM | BERT、ALBEF 文本编码器 |
| 应用场景 | **生成**任务（caption、VQA 回答、对话） | **理解**任务（分类、匹配、检索） |
一句话：  
**LM 学“写句子”，MLM 学“填空的理解”**。

在 BLIP 的 CapFilt 流程里，Captioner 和 Filter 各司其职，一个“写”、一个“审”，共同把网络爬来的噪声图文对洗成高质量训练数据：
1. Captioner（合成字幕器）  
   - **角色**：生成器。  
   - **结构**：把 BLIP 预训练好的 image-grounded text decoder 在 COCO 等人工标注数据上用 LM 损失再做轻量微调。  
   - **作用**：对每张 Web 图片自动生成多条语义丰富的合成描述 Ts，扩大高质量文本的覆盖面，弥补原始网页文本 Tw 过于简短、噪声大甚至完全无关的缺陷。
2. Filter（匹配过滤器）  
   - **角色**：判别器。  
   - **结构**：把 BLIP 预训练好的 image-grounded text encoder（ITC+ITM 头）同样在人工标注数据上微调，变成一个二分类器。  
   - **作用**：对“原始图文对 {Iw, Tw}”和“新生成图文对 {Iw, Ts}”逐一打分，只保留 ITM 预测为“匹配”且相似度得分高于阈值的样本；其余视为噪声并丢弃。  
   - **效果**：同时筛掉不相关的原始文本和偶尔跑偏的合成文本，保证进入下一轮预训练的数据都是“图-文严格对齐”的高质量对。
Filter 和 Captioner 并不是“额外设计”的新网络，而是直接把 BLIP 预训练好的 MED 里对应模块拿来**微调**：（独立训练，不共享参数）
1. Filter  
   架构：MED 中的 Image-grounded Text Encoder（ViT + 双向 Cross-Att BERT）  
   任务：ITC + ITM 二分类，判断图文是否匹配 → 输出 0/1 当“判别器”。
2. Captioner  
   架构：MED 中的 Image-grounded Text Decoder（ViT + 因果 Cross-Att GPT式解码器）  
   任务：LM 自回归生成，给图像写 caption → 当“生成器”。
二者先在原始噪声数据上随 MED 一起预训练，再在干净 COCO 数据上**轻量微调**后，分别独立用于 CapFilt 流程：Captioner 负责写描述，Filter 负责把原始或合成描述中不匹配的全部剔除。
通过“先写再审”的闭环，Captioner 不断补充优质文本，Filter 持续剔除噪声，二者迭代协作，使 BLIP 在仅 14 M 图文对规模上就超过使用 40 M 噪声数据的 ALBEF。

在 BLIP（或任何自回归生成器）里，Captioner 写完一句话时要决定“下一个词怎么挑”。  
三种策略对应 **随机性从 0→中→高**：
| 策略 | 随机性 | 机制 | 效果 |
|---|---|---|---|
| None (greedy) | 0 | 每一步直接取概率最大的词 | 输出确定、重复、单调；BLEU 高但多样性差 |
| Beam | 小 | 每步保留 k 个总概率最高的前缀（k=beam size），最后挑最优整条 | 输出仍较确定，比 greedy 丰富一点；计算量随 k 线性增 |
| Nucleus (top-p) | 大 | 每步只在累积概率 ≥ p 的最小词表（动态大小）里按归一化概率采样 | 随机采样带来多样性，可抑制低频词；p 越小越确定，p→1 接近纯采样 |
一句话：  
**None 最呆板，Beam 稍灵活，Nucleus 最会“花样造句”**。


Flamingo（一个少样本学习的视觉-语言模型）
动机：
做一个多模态的一个few-shot learning的模型，并且可以利用已经训练好的视觉模型和大预言模型
创新点
桥接强大的预训练视觉模型和大预言模型
可以处理任意图片，文本和视频混杂的数据
无缝接收图像或者视频作为输入

和BLIP的区别：
- Flamingo 的核心是“冻结视觉+冻结大语言模型，只额外学一个轻量桥接”，BLIP 则是把 ViT 和文本 Transformer 全部端到端一起训练。  
- Flamingo 在 LLM 每层中间插入了“门控交叉注意力”，让文本 token 只在当前位置瞥一眼图像；BLIP 直接把图像 token 和文本 token 拼在一起做自注意力。  
- Flamingo 天然支持“图文交错序列”，可把 4-32 对示例直接写在 prompt 里做 few-shot；BLIP 只能单图单段文本，无法利用上下文示例。  
- Flamingo 的训练目标只有“自回归生成文本”一个损失；BLIP 同时用对比、匹配、生成三个损失联合训练。  
- 想拿到同等 VQA 或字幕成绩，BLIP 必须全量微调，Flamingo 只需给极少示例即可推理，几乎零额外训练。

关键步骤：
Vision Encoder
只干三件事——ResNet 提特征 → 展平 → 投影加空间位置编码，得到 x_f 后直接丢给 Perceiver Resampler 做时空压缩即可。
# 输入: frames [T, H, W, 3]          # T 帧原始 RGB 图像
# 输出: x_f     [T, S, d]            # 已带空间位置编码的视觉 token

for t in range(T):                   # 逐帧处理
    # 1. 用 ResNet 或 NFNet 抽特征图
    f_t = resnet(frames[t])          # [h, w, c]  下采样 16× 或 32×

    # 2. 展平成空间 token 序列
    f_t = flatten(f_t)               # [h*w, c]   记 S = h*w

    # 3. 线性投影到模型隐藏维 d
    f_t = linear(f_t)                # [S, d]

    # 4. 加上可学习空间位置编码
    f_t = f_t + spatial_pos_emb      # [S, d]

    # 5. 收集为视频特征序列
    x_f[t] = f_t                     # [T, S, d]  供下游 Resampler 使用

return x_f

感知器重采样器（Perceiver Resampler）
一句话：  
**把任意长度、任意模态的输入序列“压”成固定个数、语义浓缩的向量，供下游大模型一次性消费。**
展开三点：
1. 长度归一化  
   视频帧数、图像块数都可能变，但重采样器始终输出 R 个向量，后续 Transformer 再也不用关心时空分辨率。

2. 信息精炼  
   用可学习的 latent query 做交叉注意力，逐层去噪、聚焦关键区域，把冗余像素/帧扔掉，只保留对任务有用的全局语义。

3. 模态桥梁  
   视觉特征和文本特征维度、分布不同，重采样器先把视觉侧“翻译”成 LLM 熟悉的 d 维向量，再直接送进语言模型，完成跨模态对接。

**把“可学习的 latent query”当成信息漏斗，逐层反复去“捞”视觉 token 里的有用信号，才能让 R 个固定向量真正浓缩整段视频/多帧精华。**
逐点展开：
1. 一次 cross-attention 只能让 latent 问一次视觉，相当于“瞄一眼”；  
   多层就是“瞄很多眼”，每一层都能根据上次提炼出的结果再重新挑重点，逐步去噪、聚焦关键区域。

2. 每做完一次 attention 就接 FFN，给每个 latent 增加非线性变换能力，  
   否则纯线性投影无法把复杂的视觉语义压进有限维度。

3. 采用残差（`x = x + ...`）保证梯度直接回传，避免深层漏斗训练崩掉；  
   同时让模型很容易保留“之前已提炼好的信息”，不会越压越丢。

4. 最终 R 个 latent 既见过全部时空 token，又在内部做了多步推理，  
   才能用极短序列（如 64 个向量）把任意长度视觉输入“讲清楚”，再送给大模型生成文本。
5. 可类比ViT 的 [class] token、CLIP 的文本 EOS token、MoCo 的队列原型、ALBEF/BLIP 的融合编码器里“跨模态 [CLS]”那一步：
都是用单个（或极少）固定向量，通过注意力反复去“捞”整个输入序列的信息，最后得到一个浓缩表示去做下游任务。

伪代码：
# 输入
#   x_f: 视觉特征  [T, S, d]  T=时间帧数，S=每帧空间 token 数，d=通道维度
#   time_embeddings: 时间位置编码  [T, 1, d]，给每一帧一个可学习的时间向量
#   x: 可学习的 latent query（R 个） [R, d]
#   num_layers: Perceiver 重复层数
# 输出
#   x: 精炼后的 latent 特征  [R, d]

# 1. 把时间位置编码加到对应帧上
x_f = x_f + time_embeddings          # [T, S, d] 每帧 token 共享同一时间偏置

# 2. 把时空两维展平，变成一维 token 序列
x_f = flatten(x_f)                   # [T*S, d]  方便后续注意力计算

# 3. 逐层做 cross-attention + FFN（类比[class]token）
for i in range(num_layers):
    # 3.1 Cross-Attention: 用 latent query 去 attend 视觉特征
    #     Q = x (latent),  K = V = concat[x_f, x]  视觉+latent 共同提供 KV
    x = x + attention_i(Q=x, KV=concat([x_f, x]))

    # 3.2 Feed-Forward Network：对每一个 latent 做非线性变换
    x = x + ffw_i(x)                 # 残差连接，保持梯度稳定

# 4. 返回最终精炼的 R 个 latent，作为整段视频/多帧图像的紧凑表示
return x


门控交叉注意力层（Gated XATTN-DENSE）
通过两个“可学习闸门”把视觉信号以残差方式注入 LLM 内部，同时保持原模型参数冻结，既引入跨模态信息又不破坏预训练语言先验。

y 就是LLM 主网络里某一层的隐藏状态，直接“插拔”进来——**Flamingo 把 Gated XATTN-DENSE 当成外挂模块，插到冻结的 LLM 每两层之间即可，无需改动原模型权重，这就是“可嵌入性”的体现**。

tanh 在这里就是**可学习的“水龙头”**：
1. 输出范围 (−1, 1)，先把 α 值压缩到线性区附近；  
2. 初始 α=0 ⇒ tanh(0)=0，视觉/FFW 支路完全关闭，**零扰动**保护预训练 LLM；  
3. 训练过程中 α 可正可负，tanh 给出连续缩放因子，**平滑地“开大关小”**这两条支路的信息流量；  
4. 因为流量由可学习参数控制，**像闸门一样决定“放多少水”**，所以叫“门控”。

先“锁死”再“慢慢松闸”，才能在不破坏预训练 LLM 的前提下，把全新的视觉支路安全地接进去。具有零扰动保护，渐进式融合和可逆可调的特征，具体如下：
零扰动保护
预训练语言模型已经蕴含强大先验；如果一开始就强行灌视觉信号，梯度方向可能突变，导致语言建模能力雪崩。初始闸门=0，相当于“视觉断路”，让网络先以纯文本模式继续运转。
渐进式融合
随着训练推进，α 通过梯度自动增大，视觉信息才一点点掺进来；LLM 的每一层可以逐步适应新分布，避免灾难性遗忘。
可逆可调
tanh 给出连续缩放，模型后期也可把 α 学成负值，实现“反向抑制”——哪一层不需要视觉噪声，就自动关小或关死，起到自组织特征选择的作用。

attention让每个语言 token 自己去挑“应该看哪几块视觉特征”，并把挑到的信息加权求和，拼成一个新的语言向量，每个语言 token 都拿到一份“私人定制”的视觉摘要，后续 FFW 再对这些摘要做非线性融合。（就是QKV哪个公式）
ffw这一步把每个 token 的向量单独升维 → 非线性激活 → 再降维回原尺寸，给模型一次“内部扩内存”再压缩的机会，从而学到单 token 内部更复杂的组合特征。
冻结 = 把权重变成只读，前向传播正常算，反向传播时梯度停掉，参数一步也不更新。目的是保住预训练 LLM 的海量语言先验

伪代码：
# 输入
#   y : 当前语言特征  [L, d]  (L 个 token，来自 LLM 某一层的隐藏状态)
#   x : 来自感知器重采样器的视觉特征  [R, d]  (固定 R 个向量，已浓缩全图/全视频信息)
#   alpha_xattn : 可学习的标量，初始化为 0，控制交叉注意力支路的“开度”
#   alpha_dense : 可学习的标量，初始化为 0，控制 FFW 支路的“开度”
# 输出
#   y : 融合视觉后的语言特征，维度不变  [L, d]

# 1. 门控交叉注意力：用语言 token 去 attend 视觉向量
#    tanh(alpha_xattn) 初始为 0，网络训练初期完全“关闭”视觉通路，避免扰动预训练 LLM
y = y + tanh(alpha_xattn) * attention(Q=y, KV=x)   # [L, d] ← [L, d] + λ·CrossAttn(y, x)

# 2. 门控前馈网络：对融合后的特征再做非线性变换
#    tanh(alpha_dense) 同样初始为 0，逐步“升温”打开
y = y + tanh(alpha_dense) * ffw(y)                 # [L, d] ← [L, d] + λ·FFW(y)

# 3. 原有的自注意力 + FFW 保持冻结，继续跑一遍，维持语言建模能力
y = y + frozen_self_attention(Q=y, KV=y)           # 残差，参数冻结，不改预训练权重
y = y + frozen_ffw(y)                              # 残差，参数冻结

return y                                             # 输出已带视觉语义的特征，送给下一层

在 Flamingo 的 **Gated XATTN-DENSE** 模块里，**所有“新增”参数都参与反向传播**，具体包括：
1. **交叉注意力权重**（`W_q, W_k, W_v, W_o`）  
2. **ffw 的两层线性权重**（`W_1, b_1, W_2, b_2`）  
3. **两个门控标量** `alpha_xattn` 和 `alpha_dense`
而 **LLM 原有的自注意力 + ffn 权重**（图中标成 `frozen_*`）才被冻结，**不接收梯度**。
所以更新的是：**整个外挂支路 + 门控标量**，冻结的只是原 LLM 的老参数。


BLIP-2（通过冻结图像编码器和大预言模型进行语言-图像预训练的自举方法）
简单来说，BLIP-2就是Flamingo的升级版，BLIP-2 是“极简桥接 + 双阶段冻结”，用 1/50 参数实现更高零样本性能，把“轻量即插即用”做到极致。
Flamingo 在每层插“交叉注意力+门控”给 LLM 开视觉口；BLIP-2 干脆把 ViT 和 LLM 都锁死，**在生成任务前做对齐和融合训练**，中间单设一个轻量 Q-Former 做“双语翻译”，先学视觉-查询对齐，再学查询-文本生成，两阶段结束即可零样本看图说话，使得参数量只有 Flamingo 的 1/54，却给出更高零样本 VQA 成绩

Flamingo（“边生成边注入”）
冻结 ViT → Perceiver Resampler → 门控交叉注意力层（插在 LLM 中间）→ 自回归生成
视觉侧只压一次，得到的 64 个 token 像“外挂内存”一样，被各层门控注意力反复读取。
对齐信号来自生成损失：模型必须一边写文本、一边挑视觉 token，误差回传只更新 Resampler + gate。
因此视觉-语言交互是“生成时刻的即时耦合”，没有事先学一个通用图文表示。

BLIP-2（“先对齐表示，再拿去生成”）
阶段 1：冻结 ViT → Q-Former ← 冻结文本编码器（OPT/T5-encoder 侧）
  用图文对比、图文匹配、图像字幕三大目标，先把 32 个 Learnable Query 训练成“对齐后的视觉摘要”。
阶段 2：把同一组 Query 输出直接 线性投影 → 冻结 LLM（OPT/T5-decoder 侧）
  仅训练 Q-Former 与投影矩阵，让 LLM 把这份摘要当成“软提示”去做生成。

两阶段训练
① 视觉-语言表示学习：在冻结图像编码器基础上，用图文对比、图文匹配、图像字幕生成三个目标训练 Q-Former，进行文本和视觉特征的对齐和融合，使其学会“用 Query 提取对文本最有用的视觉信息”。ITC+ITM+ITG 三任务并行，只训 Q-Former 对齐图文，统一计算损失
② 视觉到语言生成学习：把 Q-Former 输出接入冻结的 LLM，仅训练 Q-Former 与投影层，完成字幕生成、VQA、对话等任务，训练模型的多模态生成能力。

模型结构
视觉侧：冻结的 CLIP ViT 编码器，负责抽取图像特征。
语言侧：冻结的 OPT/Flan-T5 等大模型，负责文本生成与推理。
桥接模块：可学习的 Q-Former（含交叉注意力与自注意力）把 32 个可学习 Query 压缩成固定长度的“视觉摘要”，再经线性投影送入 LLM。

LLaVA(大型语言视觉助手)
关键优化：**冻结大模型范式、指令微调方法论、GPT-4 蒸馏数据**
阶段一：特征对齐预训练
冻结视觉编码和LLM，仅训练线性投影层
将图像编码后，通过投影层（Projection W）映射到语言模型输入空间，与语言指令（Language Instruction）拼接后一起送入大语言模型（Language Model）生成响应（Language Response）
具体流程：
1.Vision Encoder（通常是 CLIP 的 ViT）提取图像特征，得到视觉表示 （图像向量）。
2. Projection W 是一个线性层（或 MLP），将视觉向量映射到与语言 token 相同的维度，变成“伪词向量”。
3. 这些视觉向量与 Language Instruction（用户输入的文本指令）拼接，形成图文混合的输入序列。
4. 最终送入 Language Model（如 Vicuna/LLaMA），模型基于图文信息生成 Language Response。
阶段二：端到端微调
冻结视觉编码器，训练线性投影层和LLM

LLaVA 结构极简，三段式“视觉→投影→语言”：
视觉端：冻结 CLIP-ViT，输出网格特征。
投影层：单层/双层 MLP 把视觉特征映射到词嵌入维度，得到固定长度 image token。
语言端：冻结 Vicuna/LLaMA，自回归接收“指令文本 + image token”混合序列，直接生成回答。

BLIP-2 是“先上课再考试”——专门用对比、匹配、生成三任务把视觉摘要预训练成“图文双语通”，再送去 frozen LLM 应试；LLaVA 是“直接裸考”——视觉特征零基础上考场，靠 LLM 的扣分机制临场学会哪句图文对应，两者对齐时机、信号来源和可训范围完全不同。

具体差异列点：

1. 对齐阶段  
   BLIP-2：阶段一就用 ITC/ITM/ITG 三任务显式把 Q-Former 视觉 token 与文本空间拉近，LLM 全程冻结，对齐信号来自预训练。  
   LLaVA：阶段一只有随机投影，无对齐；真正的“图文绑定”靠阶段二生成错误回传，信号来自 LLM 的语言建模损失。
2. 可训模块  
   BLIP-2：主要训 Q-Former（188 M）+ 一个小线性投影；LLM 不动。  
   LLaVA：主要训投影层 W（几 M）+ LLM 的 LoRA；视觉编码器不动，但 LLM 部分解冻。
3. 视觉-文本交互深度  
   BLIP-2：有专门交叉注意力层，区域-单词可细粒度交互。  
   LLaVA：无交叉注意力，只靠 32 个固定视觉 token 与文本 token 在 LLM 的自注意力里“混排”后间接交互。
4. 数据与目标  
   BLIP-2：用大规模图文对先学通用表示，目标是对齐。  
   LLaVA：用 GPT-4 生成的多轮对话指令，目标是“会聊天”，对齐只是副产品。
5. 结果特性  
   BLIP-2：零样本 VQA、检索强，对话生硬。  
   LLaVA：多轮对话、详细描述流畅，零样本基准略弱，训练成本与难度低一到两个数量级。
形象比喻：  
BLIP-2 像先给视觉侧请私教（Q-Former）学会双语，再进考场；LLaVA 像把视觉侧直接扔进国外考场，靠不断被扣分现学外语——前者系统扎实，后者快捷便宜。

现在BLIP-2，Flamingo仍然有市场，而且活得比 LLaVA 更“垂直”、更“底层”。

1. 零样本/少样本科研赛道  
   BLIP-2 在 VQAv2、NoCaps、Flickr30K 等经典基准上仍领先 LLaVA 一截，可训练参数只有 LLaVA 的 1/3，却高出 8.7 %，继续做 zero-shot 视觉推理、图文检索实验时，论文里引用率依旧高。

2. 极端资源受限场景  
   188 M 可训参数 + 两阶段冻结方案，让 BLIP-2 能在单卡 24 GB 上完成训练，适合高校、医疗、工业质检等“小作坊”需要私有化部署、且对检测-计数-检索精度要求高的场合。

3. 当“视觉 Prompt 引擎”卖  
   Salesforce 已把 Q-Former 封装成 API，企业只需替换自己的冻结 LLM，就能快速获得图像描述、SKU 检索、报表生成能力，而 LLaVA 的 LoRA 版本需要客户自己准备对话数据再微调。

4. Flamingo 的长序列/多轮上下文优势  
   任意位置插帧、交错图文长文档理解，仍是 Flamingo 独家绝活；法律、金融、军事等需要“一次读 100 页扫描件 + 连续追问”场景，LLaVA 固定 32 token 的压缩表示就会丢细节，Flamingo 的交叉注意力内存机制更吃香。

5. 新硬件/新数据出现时的“冷启动”  
   当出现新传感器（红外、X 光）或新语种，BLIP-2/Flamingo 的预训练框架可以先用对比-匹配任务快速学表征，再接入任意 LLM；LLaVA 则必须等 GPT-4 先蒸馏出足够多对话数据才能微调，否则效果暴跌。

总结：  
LLaVA 胜在“聊天即插即用”，BLIP-2/Flamingo 胜在“零样本精度 + 长上下文 + 小训练成本”。只要科研还要刷榜、工业还要私有化零样本、文档还要长序列推理，这两条“老路线”就依旧有市场，而且常被后续工作当作 baseline 和底座。



LLaVA 的“扣分惩罚”只是训练机制，真正让它看起来“像 GPT-4”的是三层外挂buff：
1. 底座就是 GPT-4 级蒸馏  
   15 万条多轮对话是 GPT-4 亲自生成的，LLaVA 实质上在做**知识蒸馏**，把老师答案背下来再复述，表面简单，实则数据含金量高。

2. 评价维度被限定在“聊天、描述、VQA”  
   这些任务对细粒度检测、计数、OCR、复杂逻辑链要求低，GPT-4 的蒸馏数据恰好覆盖，所以得分高；一到需要**精准定位、数值推理、多图对比**的 benchmark，LLaVA 立刻落后 20+ 点。

3. 视觉侧仍靠 CLIP 老本  
   CLIP 已自带粗对齐，LLaVA 只需让 LLM 学会“把 CLIP 特征翻译成句子”，翻译层（W+LoRA）很薄，但**原始视觉语义是 CLIP 提前做好的**，并非从零“看图识字”。


所以 LLaVA 确实是用**高质量对话模板**做知识蒸馏，让轻量结构也能输出“GPT-4 味”的长篇回答。


**知识蒸馏**
知识蒸馏（Knowledge Distillation）就是“老师-学生”套路：
1. 老师模型（通常很大、很强，比如 GPT-4）提前学会一堆本事；
2. 用老师给出的“软答案”（概率分布、隐藏状态、特征向量，而不仅是硬标签）当作监督信号；
3. 学生模型（通常更小、更快、更省资源）在这些软信号上训练，把老师的“经验”复制到自己身上。

核心思想：  
让学生模仿老师的**输出分布**或**中间表示**，而不仅仅是模仿原始数据的真实标签。这样学生能学到老师模型里暗含的“暗知识”（各类类别之间的相似度、置信度、推理路径等），用更少的参数逼近老师的性能。

举个最小例子：  
猫狗分类，真实标签是“猫=1，狗=0”。老师模型输出[0.73, 0.27]——它觉得“猫”里还有点“狗”的影子。  
让学生去拟合这个 0.73 vs 0.27 的分布，而不是硬标签 1 vs 0，学生就学到更细腻的决策边界，效果通常比直接学硬标签更好。

端到端（End-to-End）= **“把原料直接塞进机器，出来就是最终成品”**，中间不需要人工做特征提取、规则设计或分阶段标注。

用一个比喻：  
- 非端到端：做蛋糕要先分别打蛋白、调蛋黄、烤坯子、抹奶油——每步都要人来管。  
- 端到端：把面粉、鸡蛋、糖全倒进一台魔法搅拌机，一键就出完整蛋糕。

---

为什么 LLaVA 说自己**端到端**，而 BLIP-2 不说？
1. LLaVA 的流水线  
   图像 + 问题 → 投影层 → LLM → 答案  
   所有参数（投影层+LoRA）在一次训练里**同时更新**，损失函数就是“生成答案的交叉熵”，没有中间监督，也没有分阶段标注 → **符合端到端定义**。

2. BLIP-2 的流水线  
   阶段 1：先练 Q-Former 做对比/匹配/生成——有中间任务和专门损失；  
   阶段 2：再把 Q-Former 输出接给冻结 LLM 练字幕。  
   两个目标不同、参数更新范围不同，甚至 LLM 全程不动 → **人为分阶段，不是端到端**。
总结：  
LLaVA 用单一“生成答案”损失就把视觉-语言接口+LLM 一起训练，所以宣称端到端；  
BLIP-2 靠分阶段预训练+冻结 LLM，更像传统“模块化组装”，因此不提端到端。

最终总结：
不是“指令微调 + 惩罚”让 LLaMA 本身高效，而是这套组合让**LLaMA 快速获得看图聊天的能力**，同时保持**训练成本极低**：

1. 指令微调 = 给模型一套“高分答题模板”（158k GPT-4 多轮对话），让它死记硬背“见到什么问题就该说什么话”。  
2. 惩罚机制 = 自回归语言损失：答错词就高 loss → 梯度把 LoRA 和投影层往“GPT-4 答案”方向推，实现知识蒸馏。  
3. 结果：冻结的大脑袋（LLaMA）几乎不动，只改 0.3 % 参数，一天单卡就能学会“看图说话”，推理时复现模板风格，看上去“高效”。

所以，高效的是**把视觉能力嫁接到 LLaMA 的过程**，而不是 LLaMA 本身变得更高效；它靠的是“小参数 + 好模板 + 简单惩罚”完成快速蒸馏，而不是靠惩罚机制让基础模型脱胎换骨。