# Vision Transformer (ViT) 论文分析报告

## 1. 研究问题与重要性

### 核心问题
在计算机视觉领域，能否完全抛弃卷积神经网络（CNN），仅使用 NLP 中的标准 Transformer 编码器，直接对图像进行分类，并达到或超越 CNN 的性能？

### 重要性分析

**架构统一层面**：若可行，NLP和CV可以共享同一套架构，简化大模型设计部署。

**效率提升层面**：纯 Transformer 在大数据预训练后，精度打平甚至超过 SOTA 卷积网络，却消耗更少训练资源（TPU-days ↓）。Transformer 在 NLP 已证明"数据-模型-算力"三要素同步放大可持续提效，视觉领域亟需同样的 scaling 定律。

**范式变革层面**：纯 Transformer 使图像、文本、语音统一为"序列 → token → 注意力"成为可能，为后续多模态与生成式预训练的拓展应用打下根基。

## 2. 前人做法与存在问题

### 主要方法回顾

**局部注意力方法**
- Parmar et al. (2018)：查询仍仅在局部邻域内引用自注意力，而非全局，因此感受野受限
- Hu et al. (2019) / Ramachandran et al. (2019) / Zhao et al. (2020)：使用局部多头点积自注意力块，虽然可以完全取代卷积，但仍把注意力局限在局部窗口

**稀疏注意力方法**
- Child et al. (2019) – Sparse Transformer：采用可扩展的稀疏近似来实现全局自注意力，但需要手工设计专门的稀疏模式，工程实现复杂，难以在现代硬件加速器上高效运行

**分块注意力方法**
- Weissenborn et al. (2019) / Ho et al. (2019) / Wang et al. (2020a)：通过在不同大小的块内或仅沿单轴施加注意力来降低计算量，然而这些分块/轴向策略同样需要复杂的工程调优才能在硬件上实现

**小块全注意力方法**
- Cordonnier et al. (2020)：从输入图像中提取大小为2×2斑块，并在顶部应用完全的自注意力，但**像素块太小，使得模型只能应用于小分辨率的图像**；此外，未在大规模数据集上进行预训练，未能展现与最先进 CNN 竞争的实力

**像素级方法**
- Chen et al. (2020a) – image GPT (iGPT)：在降低分辨率和颜色空间后将 Transformer 直接应用于像素，**像素 token 让序列长度爆炸、信息冗余、目标错位，三重瓶颈把模型锁死在小容量、低语义、弱梯度**；生成式预训练后线性探测仅达到 72% ImageNet 准确率，显著低于当前监督水平

### 共同痛点
1. 未彻底移除卷积，导致工程实现碎片化
2. 缺乏"大数据 + 大模型"验证，无法体现 Transformer 的 scaling 优势
3. 现代硬件加速器上效率低，难以与 ResNet 训练成本抗衡

## 3. ViT 的改进做法

### 核心思想
把图像切块当成单词，直接喂给原始 Transformer 编码器，用大规模预训练弥补归纳偏置缺失。

### 关键改进
相对前面六种方法，ViT 只改了两件事：

**粒度换 patch**：64×64 像素 → 196 个 16×16 patch，序列长度从 4k 降到 0.2k，attention 矩阵从 16M 条目降到 38k，计算量骤降。

**用大规模监督预训练代替小数据/生成式预训练**：直接把 ImageNet-21k/JFT-300M 的类别标签作为训练信号，端到端优化分类目标，而非像素预测或线性探针。

### 具体实现细节

1. **图像到序列**：将 H×W×3 图像切分成 N 个 P×P 块，拉平后线性投影为 D 维向量，得到 1D token 序列
2. **可学习分类 token**：仿BERT的[CLS]，加在序列最前端，其输出作为整张图的表示
3. **1D 位置编码**：用可学习向量表达块顺序；实验显示 2D 感知编码无显著增益
4. **标准 Transformer 编码器**：L 层多头自注意力 + MLP，残差 + LayerNorm，无任何卷积
5. **大数据预训练**：在 ImageNet-21k（14M 图）或 JFT-300M（303M 图）上先监督预训练，再下游微调
6. **高分辨率微调**：保持 patch 大小不变，序列变长；用 2D 插值扩充位置嵌入即可

## 4. 实验证明有效性

### 实验设计

**对比实验**：在 ImageNet-1k/21k/JFT-300M 上同数据同硬件比拼，ViT-H/14 达 88.55%，比 BiT-L 高 1% 以上，预训练算力仅 1/4，**证明同精度下 ViT 训练成本 2–4× 更低**。

**迁移实验**：冻结/微调后测 CIFAR、Pets、Flowers、VTAB 等 19 任务，ViT 19/20 领先，**证明预训练特征通用性强，迁移更省数据**。

**缩放实验**：固定 JFT-300M，只改模型大小与 FLOPs，ViT 精度-算力曲线斜率更陡且未饱和，CNN 已平台，**证明 ViT 值得继续放大，放大收益更高**。

**数据规模消融**：9M→303M 子集训练，小数据段 CNN 占优，>90M 后 ViT 反超，**证明数据足够大时全局 attention 优势显现，CNN 归纳偏差成天花板**。

**线性少样本探测**：冻结特征仅用 1000 张图线性分类，ViT 准确率仍领先，**证明表征本身线性可分性更好，标注成本更低**。

**可视化机制**：低层头部已关注全图，注意力距离随深度增大且与语义区域重合，**证明 ViT 确实利用全局信息，解释为何少算力也能准**。

**自监督初步**：掩码 patch 预测预训练比从零训练高 2%，**证明 Transformer 架构也适合生成式预训练，未来可进一步降低标注依赖**。

## 5. 论文点评

### 优点分析

**简洁通用**：原版 Transformer 几乎不改代码即可跑视觉，工程上极易复现与迁移。

**验证缩放定律**：数据 scaling 定律首次在视觉被系统验证，为后续大模型路线奠定信心。

**降低训练成本**：训练成本显著下降，同等精度下算力省数倍，工业界落地友好。

**统一多模态架构**：打开统一多模态大门，同一骨干、同一预训练策略即可兼容文本、图像、视频。

### 不足之处

**小数据场景局限**：小数据场景仍依赖强正则化与蒸馏（DeiT）才能追平 CNN，低资源任务并非首选。

**探索深度有限**：自监督与卷积 Hybrid 仅浅尝，未深入探讨；检测、分割等稠密任务需后续工作（ViT-Det、Swin）补充。

**位置编码简陋**：位置编码为手工 1D，插值方式简单粗暴，对极端分辨率或旋转鲁棒性有限。

**计算复杂度问题**：计算复杂度 O(N²) 随分辨率平方增长，高分辨率场景需窗口/局部注意力改良。

### 总结
ViT 用"图像切块 + 纯 Transformer + 大数据预训练"三件套，首次在视觉领域证明：只要数据够大，CNN 的归纳偏差可被学习出来，从而为视觉大模型时代揭幕。

---

## 附录：概念解释与拓展

### A.1 核心概念解释

#### 生成式预训练
先在海量无标签数据上让模型学会"生成"语言，再用少量有标签数据把模型"微调"成下游任务专家。

#### 大模型 + 大规模监督预训练
把模型参数先吹到 10^8~10^9 量级，再用千万级带标签数据狠狠训练一次，然后微调下游任务。

#### 现状（After 2022）
- 视觉进入"生成式自监督"时代（MAE、BEiT、iBOT），无需标签即可预训练大模型
- 但"大模型 + 大规模监督"仍作为强 baseline 存在，尤其在工业质检、医学影像等标签相对充裕的场景

### A.2 技术细节拓展

#### [CLS] Token 机制
[CLS] 是 BERT 放在输入最前端、专门用来"汇总"整句话信息的一个特殊向量，最终拿它做分类或排序等下游任务。

**可学习分类 token 实现步骤**：
1. 先初始化一个可学习的向量 x_cls ∈ ℝ^D（形状跟 patch token 一样）
2. 把 196 个 patch 嵌入排成矩阵 X_patch ∈ ℝ^(196×D)
3. 在最前面插一行 x_cls，得到 Z_0 = [x_cls; X_patch] ∈ ℝ^(197×D)
4. 代码实现：`torch.cat([cls_token, patch_embed], dim=1)`
5. 接着再加 1D 位置编码（同样 197 个向量），然后才把完整的 Z_0 送进 Transformer

#### 1D 位置编码详解
这一步是给一维的 patch 序列补上"空间地址"，让模型知道每个 patch 在原始图像中的左右上下顺序。

**具体做法**：
- 把 196 个 patch 拉成一条长链后，顺序就打乱了
- 为每个位置 i 分配一个可学习的向量 E_pos[i] ∈ ℝ^D
- 与 patch 嵌入逐元素相加：z_i = x_p^i E + E_pos[i]

**结果**：
- 模型通过梯度更新，自动学会"第 0 位、第 1 位 … 第 195 位"的相对/绝对坐标
- 实验里把 1D 向量换成显式的 2D 横纵坐标再 concat 或相加，精度没涨，所以作者保留最简单的 1D 可学习版本

**本质**：1D 位置编码就是给打平的 patch 序列贴"顺序标签"，让 Transformer 在全局注意力里仍能恢复二维空间关系。

### A.3 Transformer 机制深入

#### [class] token 的作用机制
[class] token 就是 ViT 给 Transformer 序列安插的"学生代表"，就像它听完所有人的发言后，把整图意见汇总交给老师（分类器）：

**初始化**：与 196 个 patch token 一起，额外生成一个可学习的向量 x[class]（维度 = D），放到序列最前面：
Z₀ = [x[class]; x_p¹E; x_p²E; …; x_p¹⁹⁶E] + E_pos

**前向传播**：经过 L 层 Transformer 后，每个 token 都看过其他所有 token。x[class] 的对应输出 z_L⁰ 自然融合了全局信息，相当于"整图摘要"。

**分类头**：只用 z_L⁰ 接一层线性层（或 MLP）→ 输出 K 维 logits → Softmax → 类别概率。其余 196 个 patch token 的输出直接丢弃，不参与分类。

#### L 层 Transformer 工作原理
L 层 Transformer 就是 L 轮"全班互通信息 + 更新自己状态"的圆桌会议；每一层都在做同一件事：让每个 token（包括[class]）把当前知道的全局消息写进自己的向量，再 refine 一次：

**1. 多头自注意力（MSA）**
每个 token 拿出三个小向量 Q/K/V，向全场所有 token 提问并收回答案，按相关性加权求和，得到"本轮全局情报"。
公式：Attention(Q,K,V) = softmax(QK^T / √d)V

**2. 残差 + LayerNorm**
把刚收到的全局情报加回原向量（残差），再做 LayerNorm，防止梯度消失/爆炸，保持分布稳定。

**3. 前馈网络（MLP） + 残差 + LayerNorm**
对每个 token 单独过两层全连接 + 非线性（GELU），把全局信息进一步映射成更高层次的抽象特征。仍是残差 + LayerNorm 收尾。

**4. 循环 L 次 = L 层**
- 第 1 层：patch 主要看到邻近几块，形成"边缘/颜色"基元
- 中间层：远距离 patch 开始交互，出现"角点/纹理/物体部件"
- 第 L 层：[class] token 已汇聚整图语义，可直接用于分类；patch token 也携带丰富的物体/背景表示，可留给分割、检测等下游任务

#### 多头自注意力机制详解
多头自注意力决定「当前 token 应该把哪些远处的信息、以及什么种类的信息」写进自己的向量，从而让下一层拿到已整合好的上下文。

**信息收集**：
每个 patch/word 用 Q 当"查询"，K 当"键"，V 当"值"→ 一次矩阵乘得到全局相关权重；权重高的位置，其 V 被大量采回，相当于把远处有用特征拷贝到自己的表示里。

**信息分类**：
- 头-0 可能专盯"颜色/纹理"局部相似性
- 头-1 关注"物体部件"空间邻接
- 头-2 捕捉"长距离指代"…
多组独立投影 = 多个专用通道，并行收集不同语义/视觉线索，避免单一阵列的表示瓶颈。

**信息融合**：
把各头采回的 V 拼接 → 再线性投影回原始维度；结果向量同时包含：
- 自身原有特征（残差会加回）
- 来自任意位置、经多种"视角"过滤后的上下文
下一层 FFN 在此基础上做非线性变换，逐步提炼高级模式。

**梯度回传时的作用**：
损失梯度通过 softmax 的权重矩阵 ∂L/∂A 反向流向 W_Q, W_K, W_V；训练过程 = 自动学习「哪些位置、哪种特征值得被关注」 → 越深，关注图越精准、语义化。

### A.4 架构理解与比喻

#### Transformer 本质
Transformer 不是某种"模型"，而是一套可插拔、可组合的乐高式算子规范：
- **唯一必需组件**：Multi-Head Attention（可 masked / 可 cross）
- **标准配件**：Add & Norm、Position-wise FFN、Positional Encoding
- **核心超参**：层数 N、头数 h、隐层 d_model、前馈扩缩比 4×

只要满足"用注意力替代递归/卷积 + 残差高速通道 + 大规模并行矩阵乘"这三条，就符合 Transformer 架构。
→ 架构中性 + 全局并行 + 可深度堆叠 = 乐高底座
→ 不同任务 = 不同插件
→ 于是"一个 Transformer，千种用法"

#### 残差连接的作用
残差（+ z）是"梯度高速公路"——让信息（和梯度）可以跳过这一层的变换，直接向后传递，从而训得动、训得快、不致梯度消失。

**具体作用**：
- **梯度畅通**：有残差时，反向传播总路径至少保留一条"恒等分支"，梯度不会被深层堆栈的连乘压缩到接近 0
- **容易学到"恒等映射"**：如果某层对当前特征已足够好，网络只需让 MSA/MLP 的权重→0，输出≈0，整体即退化为恒等，不强制改变表示
- **加速收敛、提高最终精度**：实验一致表明，去掉残差后深层 Transformer 训练损失下降缓慢，验证精度明显降低

#### 形象比喻
把向量想成一张草稿纸：
- 每层老师（注意力 + FFN）用红笔在上面补充/删除/强调内容
- 残差 + Norm 是可擦写保护膜，保证旧草稿不消失且字迹整齐
- 期末评分（损失）告诉老师哪里还错 → 下一遍草稿更完善
- 反复几轮，草稿纸上的答案从潦草到清晰，也就是「分辨率越来越强」

「上一层输出」就是残差&Norm后的向量序列；每层在前向不断「读写」全局信息，反向用梯度把「对任务有用」的改动写回参数，于是再进入下一层时向量已更精炼——如此循环，深度网络才越学越准。

#### 解码器关键机制详解

解码器包含两个核心注意力机制，它们协同工作实现序列生成：

#### 1. 掩码多头自注意力（Masked Multi-Head Self-Attention）

**出现位置**：
- 仅存在于解码器的第一层
- 在同一层里，它比普通自注意力多一块下三角掩码矩阵

**掩码矩阵结构**：
对序列长度 L，生成一个 L×L 的下三角全 1 矩阵，上三角全 0（或 -∞）：

```
1 0 0 0
1 1 0 0  
1 1 1 0
1 1 1 1
```

**实现机制**：
- 把掩码 broadcast 到 `(B, h, L, L)`
- 在 softmax 之前把 0 置成 -∞
- 确保 t 位置只能看到 0…t 的内容

**核心目的**：
保证生成第 t 个词时，模型只能利用已经生成的左侧内容，防止"偷看"未来 token。否则训练时会出现信息泄漏，推理时却看不到未来，导致效果骤降。

> **形象记忆**：老师发卷子前把下半页用订书机钉死，学生只能做当前这行。

#### 2. 交叉注意力（Cross-Attention）

**出现位置**：
- 解码器第二层（紧接在 Masked Self-Attention 之后）
- 编码器里没有此机制

**Q/K/V 来源分工**：
- **Q（查询）**：来自上一层解码器输出（已生成部分）
- **K/V（键/值）**：来自编码器最终输出（源句记忆）

**张量形状**：
```
Q: (B, h, L_tgt, d_k)     # 目标序列长度
K/V: (B, h, L_src, d_k)   # 源序列长度  
注意力矩阵: (B, h, L_tgt, L_src)
```

**工作原理**：
每生成一个目标词，都去整个源句里挑最相关的片段。

**掩码需求**：
不需要掩码，因为源句在推理前就已完全可见；要利用全部信息才能翻译/摘要准确。

**核心目的**：
实现"源→目标"动态对齐：
- 生成"machine"时，注意力权重落在源句"机器"位置
- 生成"learning"时，权重滑到"学习"位置
- 这就是 Transformer 取代 RNN-search 的精髓

> **形象记忆**：考生（解码器）写作文时，可以反复回头看原材料（编码器记忆），但只能看自己已写部分，不能提前偷看标准答案。

#### 编码器（Encoder）的作用

**任务**：把输入序列（如源语言句子）"读进去"，抽取出高层次的上下文表示。

**关键能力**：
- **双向上下文建模**：自注意力同时看左右词，得到的向量是"全局感知"的
- **多层抽象**：堆叠 N 层后，顶层每个位置都蕴含整句语义
- **产出**：一组固定长度的向量序列（常称 memory/key-value），它们不再关心具体语言形式，只保留"要表达的内容"

#### 解码器（Decoder）的作用

**任务**：根据编码器提供的"记忆"，自回归地生成目标序列（如翻译结果、摘要、回答等）。

**关键能力**：
- **掩码自注意力**：生成第 t 个词时只能看已生成的 0…t-1，保证自回归合理性
- **编码器-解码器注意力**：每一步都能"回头"去编码器输出里挑选此刻最相关的信息，实现动态对齐
- **位置逐次预测**：最后一层线性+softmax 给出词表概率，采样得到下一个词

#### 编码-解码结构是在"训练"还是"求解"？

**训练时**：
- 你把成对的 (源句, 目标句) 同时喂给模型，用掩码目标句做教师强制（teacher forcing），计算交叉熵 loss，反向传播更新参数
- 此时掩码是防止信息泄漏的考卷订书机

**推理时（求解）**：
- 模型参数已固定，没有目标句
- 你从 `<s>` 开始，一步步自回归生成，每步把已生成的部分重新喂回解码器，掩码继续保证只能看历史
- 此时掩码是自回归生成的安全轨



### A.5 高分辨率微调技术

高分辨率微调只需"patch 数变多 + 位置编码插值"两步，无需重新训练。

**具体流程**：

**1. 图像放大，patch 数自动变多**
例：
- 预训练 224×224，16×16 patch → 14×14 = 196 个 token
- 微调 512×512，同样 16×16 patch → 32×32 = 1024 个 token
序列长度从 197（含 cls）→ 1025，模型结构（D、头数、层数）完全不动。

**2. 位置编码 2D 插值**
- 把预训练得到的 197×D 位置矩阵视为 14×14 的 2D 网格（忽略 cls，可还原为 14×14×D）
- 用双线性/双三次插值直接 resize 到 32×32×D，再拉平成 1025×D（第一行留给 cls，直接复制原值）
- 插值后矩阵作为新位置编码，可学习参数保持初始化值，后续微调继续梯度更新

**代码实现细节**：
```python
patch_embed = torch.stack([x_p^i E for i in 1..196])   # (196, D)
cls_token   = x_cls.expand(1, -1)                      # (1,  D)
z_0         = torch.cat([cls_token, patch_embed], 0)   # (197, D)
z_0         = z_0 + E_pos                              # (197, D)  ← 位置编码在这里
```


> 强归纳偏置：小数据友好，但归纳偏置本身不是一段代码，也不是一个可触摸的“物体”，但它一定“长”在模型的三个可见部位里：

> 结构 · 参数 · 算法

下面给你一张“长相速查表”，看到就能认出它。

---

1️⃣ 长在“结构”里——网络拓扑就是偏置

模型	你肉眼看到的“样子”	对应的归纳偏置	
CNN	3×3 卷积核 + 权重共享滑动窗口	局部性 + 平移不变性	
RNN	隐藏状态 hₜ 只来自 hₜ₋₁ 和 xₜ	时序因果性	
Transformer	注意力矩阵遮掉上三角（causal mask）	自回归因果性	
ViT	图像先被切成 16×16 patch	局部性（仅一次）	

👉 “长什么样”= 把网络图打印出来，那些连线形状本身就是偏置。

---

2️⃣ 长在“参数”里——正则项先验

类型	公式/代码片段	先验“长相”	
L2 正则	`loss = CE + λ‖W‖²`	权重接近 0 的钟形曲线	
L1 正则	`loss = CE + λ‖W‖₁`	权重稀疏的拉普拉斯分布	
权重共享	`Conv2d(..., groups=1)` 同一核复用	参数矩阵重复块	

👉 “长什么样”= 损失函数多一项，或者参数矩阵里出现大量重复数字块。

---

3️⃣ 长在“算法”里——训练/推理流程强行加规则

场景	代码/操作	先验“长相”	
数据增强	`RandomHorizontalFlip(p=0.5)`	左右翻转不应改变类别	
早停	`EarlyStopping(patience=5)`	相信“验证集不再下降就结束”	
最大池化	`MaxPool2d(2)`	精确位置不重要	
随机失活	`Dropout(0.1)`	网络不该依赖单个神经元	

👉 “长什么样”= 训练脚本里那些 `.flip()`、`.pool()`、`.drop()` 的调用行。

---

4️⃣ 一张“全家福”——把偏置画在一张图里

```
输入图像
   │
   ├─→ [16×16 patch]       局部性偏置（ViT）
   │        │
   │        ├─→ [共享线性投影] 平移不变偏置
   │        │
   │        └─→ [+ 可学习位置向量] 位置偏置
   │
   ├─→ [3×3 Conv, 权重共享] 局部+平移偏置（CNN）
   │
   └─→ [causal mask]      因果偏置（Transformer Decoder）
```

---

#### 核心洞察

> **归纳偏置 = 人类先验知识的模型化表达**
> 可能限制模型上限
> 
> 弱归纳偏置：需要大数据学习，但潜力更大

这解释了为什么ViT论文强调"大数据预训练"的重要性——只有数据足够大，模型才能从头学会CNN内置的视觉先验。

### A.6 实验方法论

#### 实验对应关系（按论文章节/图表顺序）
1. **对比实验** → **4.2 与表 2**：同数据同硬件对比 ViT-H/14、ViT-L/16 vs BiT-L、Noisy Student，给出精度与 TPU-core-days
2. **迁移实验** → **4.1 段落 + 表 2 下游列表**：同一套预训练权重迁到 CIFAR-10/100、Pets、Flowers-102、VTAB-19 等任务并报告结果
3. **缩放实验** → **4.4 与图 5**：固定 JFT-300M，仅改模型大小/训练 epoch，绘制"平均准确率 vs 预训练计算"曲线
4. **数据规模消融** → **4.3 与图 4**：固定 ViT-B/L 架构，只用 9M → 303M 子集训练，观察 ImageNet 精度随数据量的变化
5. **线性少样本探测** → **4.3 图 4 右侧 + 正文"linear few-shot"**：冻结特征，仅用 1000 张图线性分类，对比不同数据量下的准确率
6. **可视化与内部机制** → **4.5 + 图 6/7**：分析嵌入 PCA、位置嵌入相似性、注意力距离与热图，解释模型为何有效

#### 各类实验的通用目的

**1. 对比实验**
通用目的：在相同数据、相同硬件条件下直接比较不同模型/方法的最终精度与训练成本，给出"谁更优"的量化答案。

**2. 迁移实验**
通用目的：将同一套预训练权重应用到多个下游任务，检验特征的通用性与可迁移性，衡量"一次预训练，多处复用"的价值。

**3. 缩放实验**
通用目的：固定数据和训练策略，仅改变模型大小或计算量，绘制性能-资源曲线，判断架构的扩展潜力与性价比。

**4. 数据规模消融实验**
通用目的：固定模型和训练策略，仅改变训练数据量，观察性能随数据增长的变化，量化数据对模型的重要性与架构的数据效率。

**5. 线性少样本探测**
通用目的：冻结全部特征提取器，仅用极少样本训练线性分类器，测定预训练特征的线性可分性与少样本学习能力。

**6. 可视化与内部机制分析**
通用目的：通过注意力图、特征分布等可视化手段，解释模型为何有效，验证其是否学到预期的高层语义或全局信息。

#### 正确阅读论文的方法

**先抓"实验目的"**
标题 & 首句往往给出核心问题（例如"展示 ViT 在同算力下优于 CNN"）。

**梳理"三表一图"**
- 主结果表（同类模型同指标对比）
- 消融表（单一变量变动）
- 缩放曲线图（性能-计算关系）
- 可视化图（解释性证据）

快速定位这些图表，再回正文找数字与显著性（↑↓、±std、p-value）。

**对照"控制变量"**
检查是否数据、算力、优化器、epoch 保持一致；只有模型/方法不同，结论才可信。

**看"负面结果"**
作者主动报告的失败场景（小数据 ViT 不如 ResNet）往往比正面结果更能揭示方法边界。

**关注"可复现信息"**
超参、硬件天数、随机种子、评估脚本是否开源；缺少这些，数字只能参考。

**记录"留待未来"句子**
作者自述的局限与展望，是后续研究的切入口。

### A.7 相关概念补充

#### Cordonnier vs ViT 的相似性
两者在输入处理方面的区别：
- **Cordonnier**：把图像切成 2×2 像素的小块 → 拉平 → 线性投影成向量序列
- **ViT**：把图像切成 16×16 像素的块 → 拉平 → 线性投影成向量序列
⇒ 都是 "patch → embed → 1D 序列" 这一步，没有任何卷积

